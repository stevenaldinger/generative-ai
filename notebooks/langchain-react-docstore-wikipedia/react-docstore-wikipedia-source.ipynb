{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d5d4bf-dadf-4058-8024-9da9e1ebbc89",
   "metadata": {},
   "source": [
    "# ReAct Wikipedia Source with LangChain\n",
    "\n",
    "This notebook shows how to hook up Wikipedia knowledge to an LLM using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecb0037-7968-4976-8324-9048df23d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ****************** [START] Google Cloud project settings ****************** #\n",
    "project =  os.getenv('GCP_PROJECT')\n",
    "location = os.environ.get('GCP_REGION', 'us-central1')\n",
    "# ******************* [END] Google Cloud project settings ******************* #\n",
    "\n",
    "\n",
    "# *********************** [START] Embeddings config ************************* #\n",
    "# set rate limiting options for Vertex AI embeddings\n",
    "embeddings_requests_per_minute = 100\n",
    "embeddings_num_instances_per_batch = 5\n",
    "# *********************** [END] Embeddings config *************************** #\n",
    "\n",
    "\n",
    "# *********************** [START] LLM parameter config ********************** #\n",
    "# Vertex AI model to use for the LLM\n",
    "model_name='text-bison@002'\n",
    "\n",
    "# determines the maximum amount of text output from one prompt.\n",
    "# a token is approximately four characters.\n",
    "max_output_tokens = 1024\n",
    "\n",
    "# temperature controls the degree of randomness in token selection.\n",
    "# lower temperatures are good for prompts that expect a true or\n",
    "# correct response, while higher temperatures can lead to more\n",
    "# diverse or unexpected results. With a temperature of 0 the highest\n",
    "# probability token is always selected. for most use cases, try\n",
    "# starting with a temperature of 0.2.\n",
    "temperature = 0.2\n",
    "\n",
    "# top-p changes how the model selects tokens for output. Tokens are\n",
    "# selected from most probable to least until the sum of their\n",
    "# probabilities equals the top-p value. For example, if tokens A, B, and C\n",
    "# have a probability of .3, .2, and .1 and the top-p value is .5, then the\n",
    "# model will select either A or B as the next token (using temperature).\n",
    "# the default top-p value is .8.\n",
    "top_p = 0.8\n",
    "\n",
    "# top-k changes how the model selects tokens for output.\n",
    "# a top-k of 1 means the selected token is the most probable among\n",
    "# all tokens in the modelâ€™s vocabulary (also called greedy decoding),\n",
    "# while a top-k of 3 means that the next token is selected from among\n",
    "# the 3 most probable tokens (using temperature).\n",
    "top_k = 40\n",
    "\n",
    "# how verbose the llm and langchain agent is when thinking\n",
    "# through a prompt. you're going to want this set to True\n",
    "# for development so you can debug its thought process\n",
    "verbose = True\n",
    "# *********************** [END] LLM parameter config ************************ #\n",
    "\n",
    "\n",
    "# ********************** [START] Configuration Checks *********************** #\n",
    "if not project:\n",
    "    raise Exception('GCP_PROJECT environment variable not set')\n",
    "# *********************** [END] Configuration Checks ************************ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5947d9",
   "metadata": {},
   "source": [
    "## Import and Initialize Vertex AI Client\n",
    "\n",
    "This will complain about not having cuda drivers and the GPU not being used. You can safely ignore that. If you want to use the GPU, that's possible in Linux with Docker, but you'll need to set up a non-containerized development environment to use GPUs with MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076f2cc2-ed7b-4381-ad68-e6c21b811f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 06:32:08.376992: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 06:32:08.378811: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 06:32:08.400249: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-13 06:32:08.400287: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-13 06:32:08.400307: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-13 06:32:08.404782: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 06:32:08.405265: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 06:32:08.823497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.36.0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=project, location=location)\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43797d",
   "metadata": {},
   "source": [
    "## Import LangChain\n",
    "\n",
    "This doesn't actually initialize anything, it just lets us print the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87c2a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.330\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909de17",
   "metadata": {},
   "source": [
    "## Configure LLM with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac3671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=model_name,\n",
    "    max_output_tokens=max_output_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab40f3",
   "metadata": {},
   "source": [
    "## Initialize Embeddings Function with Vertex AI\n",
    "\n",
    "There are other options for creating embeddings. I was interested in sticking with Google products here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a68651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.vertexai.VertexAIEmbeddings.html\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    requests_per_minute=embeddings_requests_per_minute,\n",
    "    num_instances_per_batch=embeddings_num_instances_per_batch,\n",
    "    model_name = \"textembedding-gecko@latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcfb57",
   "metadata": {},
   "source": [
    "## Load Wikipedia Docstore Explorer and Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b955c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.wikipedia import Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool, AgentExecutor\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "\n",
    "\n",
    "# load the wikipedia docstore with ReAct docstore explorer.\n",
    "# this provides two functions as the interface, \".search\", and \".lookup\"\n",
    "# - https://python.langchain.com/docs/integrations/document_loaders/wikipedia\n",
    "# - https://python.langchain.com/docs/modules/agents/agent_types/react_docstore\n",
    "#\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "\n",
    "wikipedia_search_tool = Tool(\n",
    "    name=\"Search\",\n",
    "    func=docstore.search,\n",
    "    description=\"Search for a term in Wikipedia. Always use this before a lookup.\",\n",
    ")\n",
    "\n",
    "wikipedia_lookup_tool = Tool(\n",
    "    name=\"Lookup\",\n",
    "    func=docstore.lookup,\n",
    "    description=\"Lookup a term in Wikipedia.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e529d8",
   "metadata": {},
   "source": [
    "## Build tool chain\n",
    "\n",
    "I don't think the order matters here since it's reasoning about which tools to call on based on the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c09e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "  wikipedia_search_tool,\n",
    "  wikipedia_lookup_tool,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723df7a0",
   "metadata": {},
   "source": [
    "## initialize agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cef1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "\n",
    "# initialize ReAct agent\n",
    "react = initialize_agent(\n",
    "  tools,\n",
    "  llm,\n",
    "  agent=AgentType.REACT_DOCSTORE,\n",
    "  verbose=verbose,\n",
    "  # https://python.langchain.com/docs/modules/agents/how_to/max_time_limit\n",
    "  max_execution_time=60,\n",
    "  # By default, the early stopping uses the force method which\n",
    "  # just returns that constant string. Alternatively, you could\n",
    "  # specify the generate method which then does one FINAL pass\n",
    "  # through the LLM to generate an output.\n",
    "  early_stopping_method=\"generate\",\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "  agent=react.agent,\n",
    "  tools=tools,\n",
    "  verbose=verbose,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534185c",
   "metadata": {},
   "source": [
    "## Ask the LLM some questions it wouldn't know about from the stock model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd0705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: I need to search Google Gemini product and find what it is.\n",
      "Action: Search[Google Gemini product]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, and Gemini Nano, it was announced on December 6, 2023, positioned as a contender to OpenAI's GPT-4.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Google's Gemini product is a family of multimodal large language models.\n",
      "Action: Finish[a family of multimodal large language models]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a family of multimodal large language models'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Google's Gemini product?\"\n",
    "agent_executor.run(question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
