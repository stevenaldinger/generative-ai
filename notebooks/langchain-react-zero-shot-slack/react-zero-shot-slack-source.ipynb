{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d5d4bf-dadf-4058-8024-9da9e1ebbc89",
   "metadata": {},
   "source": [
    "# ReAct Zero-Shot from Slack Chat History with LangChain\n",
    "\n",
    "This example reads in a Slack history export `.zip` file, creates embeddings, and creates a question-answering agent that can search the history for context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65784d9",
   "metadata": {},
   "source": [
    "## ChromaDB Persistence\n",
    "\n",
    "Each notebook that uses ChromaDB follows the same pattern for persistence.\n",
    "\n",
    "If the directory already exists that ChromaDB would be writing it's data to, it will load the existing database. If the directory does not exist, it will create a new database.\n",
    "\n",
    "If you change parameters that affect the embeddings generation (like swapping in a new `.zip` file), you'll need to delete the database directory to force a new database to be created.\n",
    "\n",
    "This can be done by running the following from the root of the repository. If the ChromaDB directory is `data/chromadb/slack_export`, you'd run the following to delete it:\n",
    "\n",
    "```sh\n",
    "rm -rf data/chromadb/slack_export\n",
    "```\n",
    "\n",
    "or if you run into permissions issues:\n",
    "\n",
    "```sh\n",
    "sudo rm -rf data/chromadb/slack_export\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, run a Slack export from Slack's web UI to get a `.zip` of your workspaces. Move that `.zip` file to the `data/slack` directory.\n",
    "\n",
    "Then configure the `slack_file_path` and `slack_workspace_url` variables to match your Slack export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecb0037-7968-4976-8324-9048df23d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ****************** [START] Google Cloud project settings ****************** #\n",
    "project =  os.getenv('GCP_PROJECT')\n",
    "location = os.environ.get('GCP_REGION', 'us-central1')\n",
    "# ******************* [END] Google Cloud project settings ******************* #\n",
    "\n",
    "\n",
    "# *********************** [START] Embeddings config ************************* #\n",
    "# set rate limiting options for Vertex AI embeddings\n",
    "embeddings_requests_per_minute = 100\n",
    "embeddings_num_instances_per_batch = 5\n",
    "# *********************** [END] Embeddings config *************************** #\n",
    "\n",
    "\n",
    "# ********************** [START] data directory config ********************** #\n",
    "from helpers.files import get_data_dir\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "chroma_db_dir = f'{data_dir}/chromadb'\n",
    "chroma_db_slack_source_dir = f'{chroma_db_dir}/slack_export'\n",
    "\n",
    "slack_dir = os.path.join(data_dir, 'slack')\n",
    "# *********************** [END] data directory config *********************** #\n",
    "\n",
    "\n",
    "# ********************** [START] LLM data config **************************** #\n",
    "from helpers.files import file_exists\n",
    "\n",
    "collection_name = 'slack-source'\n",
    "load_documents = True\n",
    "if file_exists(chroma_db_slack_source_dir):\n",
    "    load_documents = False\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 200\n",
    "# *********************** [END] LLM data config ***************************** #\n",
    "\n",
    "\n",
    "# *********************** [START] RAG parameter config ********************** #\n",
    "# experiment with:\n",
    "# - mmr\n",
    "# - similarity\n",
    "db_search_type = \"similarity\"\n",
    "db_search_kwargs = {\"k\": 20}\n",
    "# *********************** [END] RAG parameter config ************************ #\n",
    "\n",
    "\n",
    "# *********************** [START] Slack tool config ************************* #\n",
    "slack_file_path = os.path.join(slack_dir, 'eng_slack_channels_on_call.zip')\n",
    "slack_workspace_url = 'https://example-company.slack.com'\n",
    "# *********************** [END] Slack tool config *************************** #\n",
    "\n",
    "\n",
    "# *********************** [START] LLM parameter config ********************** #\n",
    "# Vertex AI model to use for the LLM\n",
    "model_name='text-bison@002'\n",
    "\n",
    "# maximum number of model responses generated per prompt\n",
    "candidate_count = 1\n",
    "\n",
    "# determines the maximum amount of text output from one prompt.\n",
    "# a token is approximately four characters.\n",
    "max_output_tokens = 2048\n",
    "\n",
    "# temperature controls the degree of randomness in token selection.\n",
    "# lower temperatures are good for prompts that expect a true or\n",
    "# correct response, while higher temperatures can lead to more\n",
    "# diverse or unexpected results. With a temperature of 0 the highest\n",
    "# probability token is always selected. for most use cases, try\n",
    "# starting with a temperature of 0.2.\n",
    "temperature = 0.2\n",
    "\n",
    "# top-p changes how the model selects tokens for output. Tokens are\n",
    "# selected from most probable to least until the sum of their\n",
    "# probabilities equals the top-p value. For example, if tokens A, B, and C\n",
    "# have a probability of .3, .2, and .1 and the top-p value is .5, then the\n",
    "# model will select either A or B as the next token (using temperature).\n",
    "# the default top-p value is .8.\n",
    "top_p = 0.8\n",
    "\n",
    "# top-k changes how the model selects tokens for output.\n",
    "# a top-k of 1 means the selected token is the most probable among\n",
    "# all tokens in the modelâ€™s vocabulary (also called greedy decoding),\n",
    "# while a top-k of 3 means that the next token is selected from among\n",
    "# the 3 most probable tokens (using temperature).\n",
    "top_k = 40\n",
    "\n",
    "# how verbose the llm and langchain agent is when thinking\n",
    "# through a prompt. you're going to want this set to True\n",
    "# for development so you can debug its thought process\n",
    "verbose = True\n",
    "# *********************** [END] LLM parameter config ************************ #\n",
    "\n",
    "\n",
    "# ********************** [START] Configuration Checks *********************** #\n",
    "if not project:\n",
    "    raise Exception('GCP_PROJECT environment variable not set')\n",
    "# *********************** [END] Configuration Checks ************************ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e9b00",
   "metadata": {},
   "source": [
    "## Import and Initialize Vertex AI Client\n",
    "\n",
    "This will complain about not having cuda drivers and the GPU not being used. You can safely ignore that. If you want to use the GPU, that's possible in Linux with Docker, but you'll need to set up a non-containerized development environment to use GPUs with MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076f2cc2-ed7b-4381-ad68-e6c21b811f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 01:36:06.203543: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-17 01:36:06.205188: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-17 01:36:06.223318: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 01:36:06.223352: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 01:36:06.223367: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 01:36:06.227519: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-17 01:36:06.228138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 01:36:06.675912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.38.1\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=project, location=location)\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fc80c",
   "metadata": {},
   "source": [
    "## Import LangChain\n",
    "\n",
    "This doesn't actually initialize anything, it just lets us print the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87c2a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.350\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b9a7c",
   "metadata": {},
   "source": [
    "## Configure LLM with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac3671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=model_name,\n",
    "    max_output_tokens=max_output_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    verbose=verbose,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9be59",
   "metadata": {},
   "source": [
    "## Initialize Embeddings Function with Vertex AI\n",
    "\n",
    "There are other options for creating embeddings. I was interested in sticking with Google products here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d22b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.vertexai.VertexAIEmbeddings.html\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    requests_per_minute=embeddings_requests_per_minute,\n",
    "    num_instances_per_batch=embeddings_num_instances_per_batch,\n",
    "    model_name = \"textembedding-gecko@latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d4e30",
   "metadata": {},
   "source": [
    "## Load Slack History into Local Vector Store (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d1c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import SlackDirectoryLoader\n",
    "\n",
    "if load_documents:\n",
    "  loader = SlackDirectoryLoader(slack_file_path, slack_workspace_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69661bd6",
   "metadata": {},
   "source": [
    "## Load and Chunk the Slack history into smaller pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d88b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "if load_documents:\n",
    "  documents = loader.load()\n",
    "\n",
    "  transformed_docs = split_docs(\n",
    "    documents,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "  )\n",
    "\n",
    "  print(f'Document count: {len(documents)}')\n",
    "  print(f'Transformed document count: {len(transformed_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d64a56",
   "metadata": {},
   "source": [
    "## Create Embeddings Database\n",
    "\n",
    "This is written with persistence and will not re-create the database if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a79096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "if load_documents:\n",
    "  # https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma.from_documents\n",
    "  db = Chroma.from_documents(\n",
    "    transformed_docs,\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=chroma_db_slack_source_dir,\n",
    "  )\n",
    "else:\n",
    "  db = Chroma(\n",
    "    persist_directory=chroma_db_slack_source_dir,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88589641",
   "metadata": {},
   "source": [
    "## Persist the Embeddings Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c475c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this would be safe to run in all circumstances but\n",
    "# it feels weird to try writing if there are no changes anyway\n",
    "if load_documents:\n",
    "  db.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8a1e5",
   "metadata": {},
   "source": [
    "## ask the database some things directly\n",
    "\n",
    "I removed the results from this cell because I don't want to expose real Slack history.\n",
    "\n",
    "- https://python.langchain.com/docs/modules/data_connection/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfce43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_db_docs(search_type, docs):\n",
    "    \"\"\"\n",
    "    Output looks like this:\n",
    "    ---\n",
    "    Matching documents (similarity): 4\n",
    "    page_content=\"slack message content here\" metadata={'channel': 'channel-name', 'source': 'https://example-company.slack.com/archives//p1701...', 'timestamp': '1701310050.508689', 'user': 'USER_ID'}\n",
    "    ...\n",
    "    \"\"\"\n",
    "    print('---')\n",
    "    print(f\"Matching documents ({search_type}): {len(docs)}\")\n",
    "\n",
    "    # print out the first 5 results\n",
    "    for doc in docs[:5]:\n",
    "        print(doc)\n",
    "\n",
    "# this will print out the raw slack message content and metadata about its source\n",
    "query = \"What should I do if an anomaly monitor resolves?\"\n",
    "docs = db.similarity_search(query)\n",
    "print_db_docs(\"similarity\", docs)\n",
    "\n",
    "docs = db.max_marginal_relevance_search(query)\n",
    "print_db_docs(\"max marginal relevance\", docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d160f5",
   "metadata": {},
   "source": [
    "## Create Retrievers\n",
    "\n",
    "One will be used to ask directly, and one will be used with a LangChain ReAct agent.\n",
    "\n",
    "The one we ask directly will be able to support returning source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9187f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def create_retrieval_qa_chain(return_source_documents=True):\n",
    "    retrieval_qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(\n",
    "            search_type=db_search_type,\n",
    "            search_kwargs=db_search_kwargs,\n",
    "        ),\n",
    "        # not supported in zero-shot ReAct, but can be enabled if you want\n",
    "        # to query directly the retrieval qa chain directly\n",
    "        #\n",
    "        return_source_documents=return_source_documents,\n",
    "    )\n",
    "\n",
    "    return retrieval_qa\n",
    "\n",
    "retrieval_qa = create_retrieval_qa_chain()\n",
    "retrieval_qa_react = create_retrieval_qa_chain(return_source_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337311a",
   "metadata": {},
   "source": [
    "## ask the retrieval qa chain some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492b9c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Query: What should I do if an anomaly monitor resolves?\n",
      "Result:  If an anomaly monitor resolves, you should first check the details of the alert in Datadog to see what triggered it and whether or not it's truly resolved or just reached a new normal where it no longer considered it an anomaly. You can also try to tune the monitor to alert when you want it, and see if you can stretch it out so you're not alerted on routine maintenance at the same time. If you're still getting false alarms, you can modify the notification settings to try to stop them.\n"
     ]
    }
   ],
   "source": [
    "def print_retrieval_qa_results(result):\n",
    "    print('---')\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Result: {result['result']}\")\n",
    "\n",
    "\n",
    "query = \"What should I do if an anomaly monitor resolves?\"\n",
    "result = retrieval_qa({'query': query})\n",
    "print_retrieval_qa_results(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805f1a0",
   "metadata": {},
   "source": [
    "## Configure Retrieval Augmented Generation Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2c8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def create_prompt(question):\n",
    "    prompt_rag_statement = f\"\"\"\n",
    "    Refer to TeamSnap Slack Message History tool for the following question regarding TeamSnap services.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "    return prompt_rag_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a65c0c",
   "metadata": {},
   "source": [
    "## Build tool chain\n",
    "\n",
    "This will provide knowledge to the ReAct agent from the Slack history embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a5766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentExecutor\n",
    "\n",
    "tools = [\n",
    "  Tool(\n",
    "    name=\"Read TeamSnap Slack Message History\",\n",
    "    func=retrieval_qa_react.run,\n",
    "    description=\"Useful for looking up context related to TeamSnap and TeamSnap systems.\",\n",
    "  ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447fbcb",
   "metadata": {},
   "source": [
    "## Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a5efa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "\n",
    "# initialize ReAct agent\n",
    "react = initialize_agent(\n",
    "  tools,\n",
    "  llm,\n",
    "  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "  verbose=True,\n",
    "  # https://python.langchain.com/docs/modules/agents/how_to/max_time_limit\n",
    "  max_execution_time=60,\n",
    "  # By default, the early stopping uses the force method which\n",
    "  # just returns that constant string. Alternatively, you could\n",
    "  # specify the generate method which then does one FINAL pass\n",
    "  # through the LLM to generate an output.\n",
    "  early_stopping_method=\"generate\",\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "  agent=react.agent,\n",
    "  tools=tools,\n",
    "  verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b02283",
   "metadata": {},
   "source": [
    "## Ask something that requires context from Slack history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e997cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should check TeamSnap Slack Message History to see if there are any relevant messages about what to do if an anomaly monitor resolves.\n",
      "Action: Read TeamSnap Slack Message History\n",
      "Action Input: anomaly monitor resolves\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m If you go to datadog and click on \"Monitors\" or \"manage monitors\" in the drop down, it'll take you to a big list of all our monitors.\n",
      "if you copy and paste from the pagerduty alert so you're sure you're looking at the right one (there's tons of rabbit monitors)\n",
      "when you get to the specific monitor's page, you can see the details of what triggered it and whether or not it's truly resolved or just reached a new normal where it no longer considered it an anomaly\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Go to Datadog, click on \"Monitors\" or \"manage monitors\" in the drop down, and find the specific monitor that triggered the alert. Check the details of what triggered it and whether or not it's truly resolved or just reached a new normal where it no longer considered it an anomaly.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Go to Datadog, click on \"Monitors\" or \"manage monitors\" in the drop down, and find the specific monitor that triggered the alert. Check the details of what triggered it and whether or not it\\'s truly resolved or just reached a new normal where it no longer considered it an anomaly.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What should I do if an anomaly monitor resolves?\"\n",
    "\n",
    "question = create_prompt(query)\n",
    "agent_executor.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
