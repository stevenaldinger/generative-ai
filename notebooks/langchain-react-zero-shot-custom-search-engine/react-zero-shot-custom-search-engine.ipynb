{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d5d4bf-dadf-4058-8024-9da9e1ebbc89",
   "metadata": {},
   "source": [
    "# ReAct Zero-Shot Custom Search Engine with LangChain and Google's Programmable Search Engine\n",
    "\n",
    "This uses Google's programmable search engine to search Riot's official Valorant sites and [GamerGirl](https://gamergirl.gg/) for supporting information, since Riot has not historically been the best at making certain things accessible.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure to set up your own Programmable Search Engine and add the `GOOGLE_CSE_ID` variable to the [.env](../../.env) file before running this example. If you set that variable after starting your development environment, you'll need to restart (`make stop; make start;`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d6b81",
   "metadata": {},
   "source": [
    "## Configure variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbaf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ****************** [START] Google Cloud project settings ****************** #\n",
    "project =  os.getenv('GCP_PROJECT')\n",
    "location = os.environ.get('GCP_REGION', 'us-central1')\n",
    "\n",
    "google_api_key =  os.getenv('GOOGLE_API_KEY')\n",
    "google_cse_id =  os.getenv('GOOGLE_CSE_ID')\n",
    "# ******************* [END] Google Cloud project settings ******************* #\n",
    "\n",
    "\n",
    "# *********************** [START] LLM parameter config ********************** #\n",
    "# Vertex AI model to use for the LLM\n",
    "model_name='text-bison@002'\n",
    "\n",
    "# maximum number of model responses generated per prompt\n",
    "candidate_count = 1\n",
    "\n",
    "# determines the maximum amount of text output from one prompt.\n",
    "# a token is approximately four characters.\n",
    "max_output_tokens = 1024\n",
    "# max_output_tokens = 256\n",
    "\n",
    "# temperature controls the degree of randomness in token selection.\n",
    "# lower temperatures are good for prompts that expect a true or\n",
    "# correct response, while higher temperatures can lead to more\n",
    "# diverse or unexpected results. With a temperature of 0 the highest\n",
    "# probability token is always selected. for most use cases, try\n",
    "# starting with a temperature of 0.2.\n",
    "temperature = 0.2\n",
    "\n",
    "# top-p changes how the model selects tokens for output. Tokens are\n",
    "# selected from most probable to least until the sum of their\n",
    "# probabilities equals the top-p value. For example, if tokens A, B, and C\n",
    "# have a probability of .3, .2, and .1 and the top-p value is .5, then the\n",
    "# model will select either A or B as the next token (using temperature).\n",
    "# the default top-p value is .8.\n",
    "top_p = 0.8\n",
    "\n",
    "# top-k changes how the model selects tokens for output.\n",
    "# a top-k of 1 means the selected token is the most probable among\n",
    "# all tokens in the model’s vocabulary (also called greedy decoding),\n",
    "# while a top-k of 3 means that the next token is selected from among\n",
    "# the 3 most probable tokens (using temperature).\n",
    "top_k = 40\n",
    "\n",
    "# how verbose the llm and langchain agent is when thinking\n",
    "# through a prompt. you're going to want this set to True\n",
    "# for development so you can debug its thought process\n",
    "verbose = True\n",
    "# *********************** [END] LLM parameter config ************************ #\n",
    "\n",
    "\n",
    "# ********************** [START] Configuration Checks *********************** #\n",
    "if not project or not google_api_key or not google_cse_id:\n",
    "    raise Exception(f\"\"\"All of the following environment variables must be set:\n",
    "                    - GCP_PROJECT: {project}\n",
    "                    - GOOGLE_API_KEY: {google_api_key}\n",
    "                    - GOOGLE_CSE_ID: {google_cse_id}\n",
    "                    \"\"\")\n",
    "# *********************** [END] Configuration Checks ************************ #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6c57e",
   "metadata": {},
   "source": [
    "## Import and Initialize Vertex AI Client\n",
    "\n",
    "This will complain about not having cuda drivers and the GPU not being used. You can safely ignore that. If you want to use the GPU, that's possible in Linux with Docker, but you'll need to set up a non-containerized development environment to use GPUs with MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c646ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 02:22:23.417435: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-17 02:22:23.419137: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-17 02:22:23.438270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 02:22:23.438302: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 02:22:23.438317: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 02:22:23.442621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-17 02:22:23.443439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 02:22:23.925060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.38.1\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=project, location=location)\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531432a2",
   "metadata": {},
   "source": [
    "## Import LangChain\n",
    "\n",
    "This doesn't actually initialize anything, it just lets us print the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9694cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.350\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73da165",
   "metadata": {},
   "source": [
    "## Configure LLM with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63690b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=model_name,\n",
    "    max_output_tokens=max_output_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    verbose=verbose,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4735f1",
   "metadata": {},
   "source": [
    "## Create Help, FAQ, Support, and Tutorials Search Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d1cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search_valorant = GoogleSearchAPIWrapper(\n",
    "  google_cse_id=google_cse_id,\n",
    "  google_api_key=google_api_key,\n",
    "  # https://python.langchain.com/docs/integrations/tools/google_search#number-of-results\n",
    "  # You can use the k parameter to set the number of results\n",
    "  k=5,\n",
    ")\n",
    "\n",
    "google_search_engine_valorant = Tool(\n",
    "    name=\"Valorant Help, Support, Tutorials, and Frequently Asked Questions Search\",\n",
    "    description=f\"\"\"\n",
    "Search the help, support, tutorials, and frequently asked questions for Valorant.\n",
    "This tool should be used if a question is about Valorant.\n",
    "\"\"\".strip(),\n",
    "    func=google_search_valorant.run,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a938a",
   "metadata": {},
   "source": [
    "## Build tool chain\n",
    "\n",
    "Add our Valorant search engine to the agents available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70e8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "  google_search_engine_valorant,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95fe49",
   "metadata": {},
   "source": [
    "## Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7120fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent, AgentExecutor\n",
    "\n",
    "# initialize ReAct agent\n",
    "react = initialize_agent(\n",
    "  tools,\n",
    "  llm,\n",
    "  # the wikipedia tool is structured, so this agent type gives\n",
    "  # it the ability to use both wikipedia and our simpler tools\n",
    "  agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "  verbose=True,\n",
    "  # https://python.langchain.com/docs/modules/agents/how_to/max_time_limit\n",
    "  max_execution_time=60,\n",
    "  # By default, the early stopping uses the force method which\n",
    "  # just returns that constant string. Alternatively, you could\n",
    "  # specify the generate method which then does one FINAL pass\n",
    "  # through the LLM to generate an output.\n",
    "  early_stopping_method=\"generate\",\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "  agent=react.agent,\n",
    "  tools=tools,\n",
    "  verbose=True,\n",
    "  handle_parsing_errors=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366564aa",
   "metadata": {},
   "source": [
    "## Ask the agent some questions about recent Valorant updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7aa23be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Valorant Help, Support, Tutorials, and Frequently Asked Questions Search\",\n",
      "  \"action_input\": \"valorant latest patch notes\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVALORANT Patch Notes 7.12. It's the last Patch for 2023 and Episode 7! See Article ... Riot Games presents VALORANT: a 5v5 character-based tactical FPS where precise gunplay meets unique agent abilities. Learn about VALORANT and its stylish ... Riot Games presents VALORANT: a 5v5 character-based tactical FPS where precise gunplay meets unique agent abilities. Learn about VALORANT and its stylish cast. Nov 14, 2023 ... Deadlock · Anyone caught by GravNet will now be debuffed indefinitely until they remove it · Anyone caught by GravNet will now be affected by ... Oct 31, 2023 ... Aside from all the new things released at the beginning of a new Act, we also have some Agent updates below. Patch_Notes_Highlights_7_09.jpg ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The latest Valorant patch, version 7.12, was released on November 14, 2023. It includes a new map called \\\"Deadlock\\\", agent updates, and various bug fixes and improvements.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The latest Valorant patch, version 7.12, was released on November 14, 2023. It includes a new map called \"Deadlock\", agent updates, and various bug fixes and improvements.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What updates were released in the latest Valorant patch?\"\n",
    "agent_executor.run(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
