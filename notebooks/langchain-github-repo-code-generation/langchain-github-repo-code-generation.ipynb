{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d5d4bf-dadf-4058-8024-9da9e1ebbc89",
   "metadata": {},
   "source": [
    "# Remote GitHub Repo Context For Code Generation with LangChain\n",
    "\n",
    "This example shows how to read in an entire code base, generate embeddings, store those embeddings in a persistent local database, query the database directly for relevant pieces of text, and then how to use the database as a source for an LLM hooked into LangChain to create a code generator with awareness of the repo's style and general context.\n",
    "\n",
    "This was a work-in-progress experiment I abandoned to try to build a framework for generating complex, multiple-file outputs. If something seems strange to you, especially in the prompts, follow your intuition and don't blindly trust the example.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure your `GITHUB_TOKEN` is set in the [../../.env](../../.env) file. If you set that variable after starting your development environment, you'll need to restart (`make stop; make start;`).\n",
    "\n",
    "Make sure to set the `github_repo` variable to one that you have access to in the `Configure variables` section below.\n",
    "\n",
    "## References\n",
    "- [Rubens Zimbres's Code Generation RAG Medium article](https://medium.com/@rubenszimbres/code-generation-using-retrieval-augmented-generation-langchain-861e3c1a1a53)\n",
    "- [code generation prompts (`code-bison`)](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-generation-prompts)\n",
    "- [code chat prompts (`codechat-bison`)](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-chat-prompts)\n",
    "- [test code chat prompts (`codechat-bison`)](https://cloud.google.com/vertex-ai/docs/generative-ai/code/test-code-chat-prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b50383",
   "metadata": {},
   "source": [
    "## ChromaDB Persistence\n",
    "\n",
    "Each notebook that uses ChromaDB follows the same pattern for persistence.\n",
    "\n",
    "If the directory already exists that ChromaDB would be writing it's data to, it will load the existing database. If the directory does not exist, it will create a new database.\n",
    "\n",
    "If you change parameters that affect the embeddings generation (like swapping in a new PDF file), you'll need to delete the database directory to force a new database to be created.\n",
    "\n",
    "This can be done by running the following from the root of the repository. If the ChromaDB directory is `data/chromadb/remote_repository`, you'd run the following to delete it:\n",
    "\n",
    "```sh\n",
    "rm -rf data/chromadb/remote_repository\n",
    "```\n",
    "\n",
    "or if you run into permissions issues:\n",
    "\n",
    "```sh\n",
    "sudo rm -rf data/chromadb/remote_repository\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e25bc",
   "metadata": {},
   "source": [
    "## Configure variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3261cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ****************** [START] Google Cloud project settings ****************** #\n",
    "project =  os.getenv('GCP_PROJECT')\n",
    "location = os.environ.get('GCP_REGION', 'us-central1')\n",
    "# ******************* [END] Google Cloud project settings ******************* #\n",
    "\n",
    "\n",
    "# *********************** [START] GitHub Repo Config ************************ #\n",
    "github_token = os.getenv('GITHUB_TOKEN')\n",
    "github_repo = \"imup-io/front-end\"\n",
    "# *********************** [END] GitHub Repo Config ************************** #\n",
    "\n",
    "\n",
    "# *********************** [START] Embeddings config ************************* #\n",
    "# set rate limiting options for Vertex AI embeddings\n",
    "embeddings_requests_per_minute = 100\n",
    "embeddings_num_instances_per_batch = 5\n",
    "# *********************** [END] Embeddings config *************************** #\n",
    "\n",
    "\n",
    "# ********************** [START] data directory config ********************** #\n",
    "# local directory to write chroma db persistence to\n",
    "# or pull files like PDFs from to create embeddings\n",
    "from helpers.files import get_data_dir\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "chroma_db_dir = f'{data_dir}/chromadb'\n",
    "chroma_db_remote_repo_dir = f'{chroma_db_dir}/remote_repository'\n",
    "# *********************** [END] data directory config *********************** #\n",
    "\n",
    "\n",
    "# ********************** [START] LLM data config **************************** #\n",
    "from helpers.files import file_exists\n",
    "\n",
    "collection_name = 'remote-repository'\n",
    "load_documents = True\n",
    "if file_exists(chroma_db_remote_repo_dir):\n",
    "    load_documents = False\n",
    "# *********************** [END] LLM data config ***************************** #\n",
    "\n",
    "\n",
    "# ********************** [START] repository config ************************** #\n",
    "github_repo = \"imup-io/front-end\"\n",
    "repo_dir = f\"{data_dir}/repositories/{github_repo}\"\n",
    "# *********************** [END] repository config **************************** #\n",
    "\n",
    "\n",
    "# *********************** [START] LLM parameter config ********************** #\n",
    "# Vertex AI model to use for the LLM\n",
    "model_name='code-bison@latest'\n",
    "# model_name='code-bison-32k@latest'\n",
    "\n",
    "# maximum number of model responses generated per prompt\n",
    "candidate_count = 1\n",
    "\n",
    "# determines the maximum amount of text output from one prompt.\n",
    "# a token is approximately four characters.\n",
    "max_output_tokens = 1024\n",
    "\n",
    "# temperature controls the degree of randomness in token selection.\n",
    "# lower temperatures are good for prompts that expect a true or\n",
    "# correct response, while higher temperatures can lead to more\n",
    "# diverse or unexpected results. With a temperature of 0 the highest\n",
    "# probability token is always selected. for most use cases, try\n",
    "# starting with a temperature of 0.2.\n",
    "temperature = 0.2\n",
    "\n",
    "# top-p changes how the model selects tokens for output. Tokens are\n",
    "# selected from most probable to least until the sum of their\n",
    "# probabilities equals the top-p value. For example, if tokens A, B, and C\n",
    "# have a probability of .3, .2, and .1 and the top-p value is .5, then the\n",
    "# model will select either A or B as the next token (using temperature).\n",
    "# the default top-p value is .8.\n",
    "top_p = 0.8\n",
    "\n",
    "# top-k changes how the model selects tokens for output.\n",
    "# a top-k of 1 means the selected token is the most probable among\n",
    "# all tokens in the modelâ€™s vocabulary (also called greedy decoding),\n",
    "# while a top-k of 3 means that the next token is selected from among\n",
    "# the 3 most probable tokens (using temperature).\n",
    "top_k = 40\n",
    "\n",
    "# how verbose the llm and langchain agent is when thinking\n",
    "# through a prompt. you're going to want this set to True\n",
    "# for development so you can debug its thought process\n",
    "verbose = True\n",
    "# *********************** [END] LLM parameter config ************************ #\n",
    "\n",
    "\n",
    "# ********************** [START] Configuration Checks *********************** #\n",
    "if not project:\n",
    "    raise Exception('GCP_PROJECT environment variable not set')\n",
    "\n",
    "if not github_token:\n",
    "    raise Exception('GITHUB_TOKEN environment variable not set')\n",
    "# *********************** [END] Configuration Checks ************************ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd5980",
   "metadata": {},
   "source": [
    "## Import and Initialize Vertex AI Client\n",
    "\n",
    "This will complain about not having cuda drivers and the GPU not being used. You can safely ignore that. If you want to use the GPU, that's possible in Linux with Docker, but you'll need to set up a non-containerized development environment to use GPUs with MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45945c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 01:38:09.069447: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-16 01:38:09.071158: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-16 01:38:09.090029: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-16 01:38:09.090056: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-16 01:38:09.090072: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-16 01:38:09.094264: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-16 01:38:09.094714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-16 01:38:09.567590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.38.1\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=project, location=location)\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14611a01",
   "metadata": {},
   "source": [
    "## Import LangChain\n",
    "\n",
    "This doesn't actually initialize anything, it just lets us print the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f15ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.350\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaedee",
   "metadata": {},
   "source": [
    "## Configure LLM with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=model_name,\n",
    "    max_output_tokens=max_output_tokens,\n",
    "    temperature=temperature,\n",
    "    # top_p=top_p,\n",
    "    # top_k=top_k,\n",
    "    verbose=verbose,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8970e3",
   "metadata": {},
   "source": [
    "## Initialize Embeddings Function with Vertex AI\n",
    "\n",
    "There are other options for creating embeddings. I was interested in sticking with Google products here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.vertexai.VertexAIEmbeddings.html\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    requests_per_minute=embeddings_requests_per_minute,\n",
    "    num_instances_per_batch=embeddings_num_instances_per_batch,\n",
    "    model_name = \"textembedding-gecko@latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050c761",
   "metadata": {},
   "source": [
    "## Prompt Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142f3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(topic):\n",
    "  prompt = \"\"\"\n",
    "Assume the role of an expert software developer and suggest code for the final description.\n",
    "\n",
    "\n",
    "// [START_GENERATION_SUMMARY] React \"Alert\" Component Interface\n",
    "// - description : React functional component interface for an \"Alert\"\n",
    "//                 written using Ant Design v5.\n",
    "// - file path   : src/components/Alert/Alert.interface.tsx\n",
    "// - language    : typescript\n",
    "// [END_GENERATION_SUMMARY] React \"Alert\" Component Interface\n",
    "//\n",
    "// [START_GENERATED_CODE] React \"Alert\" Component Interface\n",
    "\"\"\"\n",
    "\n",
    "  return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "# Crawls a GitHub repository and returns a list of all ts or tsx files in the repository\n",
    "def crawl_github_repo(url, is_sub_dir, access_token = f\"{github_token}\"):\n",
    "\n",
    "    ignore_list = ['__init__.py']\n",
    "\n",
    "    if not is_sub_dir:\n",
    "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
    "    else:\n",
    "        api_url = url\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    files = []\n",
    "\n",
    "    contents = response.json()\n",
    "\n",
    "    for item in contents:\n",
    "        # if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.py') or item['name'].endswith('.ipynb')):\n",
    "        if item['type'] == 'file' and item['name'] not in ignore_list and (item['name'].endswith('.ts') or item['name'].endswith('.tsx')):\n",
    "            files.append(item['html_url'])\n",
    "        elif item['type'] == 'dir' and not item['name'].startswith(\".\"):\n",
    "            sub_files = crawl_github_repo(item['url'],True)\n",
    "            time.sleep(.1)\n",
    "            files.extend(sub_files)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write discovered files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 828 code files in imup-io/front-end\n"
     ]
    }
   ],
   "source": [
    "from helpers.files import make_dir_if_not_exists\n",
    "\n",
    "files_discovered_file_path = f\"{data_dir}/code-gen-rag-repo-scrape/{github_repo}_files.txt\"\n",
    "make_dir_if_not_exists(files_discovered_file_path)\n",
    "\n",
    "code_files_urls = crawl_github_repo(github_repo, False, github_token)\n",
    "\n",
    "print(f\"Found {len(code_files_urls)} code files in {github_repo}\")\n",
    "\n",
    "# Write list to a file so you do not have to download each time\n",
    "with open(files_discovered_file_path, \"w\") as f:\n",
    "    for item in code_files_urls:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract everything in URLs\n",
    "\n",
    "I left some commented out code in here from the [Rubens Zimbres's Code Generation RAG Medium article](https://medium.com/@rubenszimbres/code-generation-using-retrieval-augmented-generation-langchain-861e3c1a1a53) I was working off of just in case it's useful to someone to be aware of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='// jest-dom adds custom jest matchers for asserting on DOM nodes.\\n// allows you to do things like:\\n// expect(element).toHaveTextContent(/react/i)\\n// learn more: https://github.com/testing-library/jest-dom\\n// get a \"document undefined\" error without this: https://jestjs.io/docs/configuration#testenvironment-string\\nimport \\'@testing-library/jest-dom\\'\\n', metadata={'url': 'https://github.com/imup-io/front-end/blob/main/jest.setup.ts', 'file_index': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "import nbformat\n",
    "import json\n",
    "\n",
    "# Extracts the python code from an .ipynb file from github\n",
    "# def extract_python_code_from_ipynb(github_url,cell_type = \"code\"):\n",
    "#     raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "\n",
    "#     response = requests.get(raw_url)\n",
    "#     response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "#     notebook_content = response.text\n",
    "\n",
    "#     notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "#     python_code = None\n",
    "\n",
    "#     for cell in notebook.cells:\n",
    "#         if cell.cell_type == cell_type:\n",
    "#           if not python_code:\n",
    "#             python_code = cell.source\n",
    "#           else:\n",
    "#             python_code += \"\\n\" + cell.source\n",
    "\n",
    "#     return python_code\n",
    "\n",
    "# Extracts the python code from an .py file from github\n",
    "def extract_code_from_file(github_url, github_token = f\"{github_token}\"):\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"Authorization\": f\"Bearer {github_token}\"\n",
    "    }\n",
    "\n",
    "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "\n",
    "    response = requests.get(raw_url, headers=headers)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    code = response.text\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "with open(files_discovered_file_path) as f:\n",
    "    code_files_urls = f.read().splitlines()\n",
    "\n",
    "code_strings = []\n",
    "\n",
    "for i in range(0, len (code_files_urls)):\n",
    "    if code_files_urls[i].endswith(\".ipynb\"):\n",
    "        print(\"Skipping .ipynb file\")\n",
    "\n",
    "        continue\n",
    "\n",
    "    if code_files_urls[i].endswith(\".ts\") or code_files_urls[i].endswith(\".tsx\"):\n",
    "        content = extract_code_from_file(code_files_urls[i])\n",
    "        doc = Document(page_content=content, metadata= {\"url\": code_files_urls[i], \"file_index\":i})\n",
    "        code_strings.append(doc)\n",
    "\n",
    "code_strings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# Chunk code strings\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.TS, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.split_documents(code_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3289f",
   "metadata": {},
   "source": [
    "## Create Embeddings Database\n",
    "\n",
    "This is written with persistence and will not re-create the database if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "if load_documents:\n",
    "  # https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma.from_documents\n",
    "  db = Chroma.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=chroma_db_remote_repo_dir,\n",
    "  )\n",
    "else:\n",
    "  db = Chroma(\n",
    "    persist_directory=chroma_db_remote_repo_dir,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever.\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",  # Also test \"similarity\", \"mmr\"\n",
    "    search_kwargs={\"k\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running prompts (non-RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```tsx\n",
      "import React from \"react\";\n",
      "import { Button } from \"antd\";\n",
      "\n",
      "const MyButton: React.FC = () => {\n",
      "  return (\n",
      "    <Button type=\"primary\">Click me!</Button>\n",
      "  );\n",
      "};\n",
      "\n",
      "export default MyButton;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "user_question = \"Create a typescript React 'Button' component using Ant Design.\"\n",
    "\n",
    "# Zero Shot prompt template\n",
    "prompt_zero_shot = \"\"\"\n",
    "    You are a proficient React and typescript developer. Respond with the syntactically correct & concise code for to the question below.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Output Code :\n",
    "    \"\"\"\n",
    "\n",
    "prompt_prompt_zero_shot = PromptTemplate(\n",
    "  input_variables=[\"question\"],\n",
    "  template=prompt_zero_shot,\n",
    ")\n",
    "\n",
    "response = llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running prompts (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```jsx\n",
      "import React, { FC } from 'react';\n",
      "import { ImUpLogo } from '@components/ImUpLogo';\n",
      "import { Tooltip } from 'antd';\n",
      "\n",
      "export const ImUpLogoButton: FC = () => {\n",
      "  const [rotate, setRotate] = useState(false);\n",
      "\n",
      "  const handleClick = () => {\n",
      "    setRotate(!rotate);\n",
      "  };\n",
      "\n",
      "  return (\n",
      "    <Tooltip title=\"ImUp.io Logo\">\n",
      "      <ImUpLogo\n",
      "        onClick={handleClick}\n",
      "        style={{ transform: `rotate(${rotate ? 90 : 0}deg)` }}\n",
      "        tooltipText=\"ImUp.io\"\n",
      "      />\n",
      "    </Tooltip>\n",
      "  );\n",
      "};\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# You are a proficient React and typescript developer. Respond with the syntactically correct & concise code for to the question below. Make sure you follow these rules:\n",
    "# 1. Use context to understand the APIs and how to use it & apply.\n",
    "# 2. Do not add license information to the output code.\n",
    "# 3. Do not include colab code in the output.\n",
    "# 4. Ensure all the requirements in the question are met.\n",
    "\n",
    "# RAG template\n",
    "prompt_RAG = \"\"\"\n",
    "    You are a proficient React and typescript developer. Respond with the syntactically correct & concise code for to the question below. Make sure you follow these rules:\n",
    "    1. Use context from imup-io/front-end GitHub repository to understand the interfaces to components and how to use it & apply.\n",
    "    2. Do not add license information to the output code.\n",
    "    3. Ensure all the requirements in the question are met.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Helpful Response :\n",
    "    \"\"\"\n",
    "\n",
    "prompt_RAG_tempate = PromptTemplate(\n",
    "    template=prompt_RAG, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm, prompt=prompt_RAG_tempate, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "# user_question = \"Create a typescript React 'Button' component using Ant Design.\"\n",
    "user_question = \"Create a typescript React component using Ant Design and imup-io/front-end repository's ImUpLogo component. The component should be named 'ImUpLogoButton' and should rotate the logo 90 degrees on click. The ImUpLogo component's tooltip should say 'ImUp.io' and the ImUpLogoButton component's tooltip should say 'ImUp.io Logo'.\"\n",
    "\n",
    "results = qa_chain({\"query\": user_question})\n",
    "print(results[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple prompts back to back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 9'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a memory object\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Respond to the user\n",
    "chain.predict(input='Create a python function that sums two variables a and b.')\n",
    "chain.predict(input='Now, run this function with a=2 and b=7 and give me only the result')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
