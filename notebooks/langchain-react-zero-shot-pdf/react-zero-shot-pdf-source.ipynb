{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d5d4bf-dadf-4058-8024-9da9e1ebbc89",
   "metadata": {},
   "source": [
    "# ReAct Zero-Shot from PDF Source with LangChain\n",
    "\n",
    "This notebook shows how to turn a PDF into embeddings, store those embeddings in a persistent local database, query the database directly for relevant pieces of text, and then how to use the database as a source for an LLM hooked into LangChain to create a zero-shot ReAct agent you can ask questions to.\n",
    "\n",
    "The PDF used in the example is the World Pool-Billiard Association's rule book. You can find the source [here](https://wpapool.com/wp-content/uploads/2017/03/WPA_New_Rules_01MAR2016-fixed-spelling.pdf).\n",
    "\n",
    "I removed the table of contents pages from the PDF because it was obfuscating the vector search and messing with the ReAct agent results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83261f9",
   "metadata": {},
   "source": [
    "## ChromaDB Persistence\n",
    "\n",
    "Each notebook that uses ChromaDB follows the same pattern for persistence.\n",
    "\n",
    "If the directory already exists that ChromaDB would be writing it's data to, it will load the existing database. If the directory does not exist, it will create a new database.\n",
    "\n",
    "If you change parameters that affect the embeddings generation (like swapping in a new PDF file), you'll need to delete the database directory to force a new database to be created.\n",
    "\n",
    "This can be done by running the following from the root of the repository. If the ChromaDB directory is `data/chromadb/pdf_source`, you'd run the following to delete it:\n",
    "\n",
    "```sh\n",
    "rm -rf data/chromadb/pdf_source\n",
    "```\n",
    "\n",
    "or if you run into permissions issues:\n",
    "\n",
    "```sh\n",
    "sudo rm -rf data/chromadb/pdf_source\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecb0037-7968-4976-8324-9048df23d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ****************** [START] Google Cloud project settings ****************** #\n",
    "project =  os.getenv('GCP_PROJECT')\n",
    "location = os.environ.get('GCP_REGION', 'us-central1')\n",
    "# ******************* [END] Google Cloud project settings ******************* #\n",
    "\n",
    "\n",
    "# *********************** [START] Embeddings config ************************* #\n",
    "# set rate limiting options for Vertex AI embeddings\n",
    "embeddings_requests_per_minute = 100\n",
    "embeddings_num_instances_per_batch = 5\n",
    "# *********************** [END] Embeddings config *************************** #\n",
    "\n",
    "\n",
    "# ********************** [START] data directory config ********************** #\n",
    "from helpers.files import get_data_dir\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "chroma_db_dir = f'{data_dir}/chromadb'\n",
    "chroma_db_pdf_source_dir = f'{chroma_db_dir}/pdf_source'\n",
    "\n",
    "pdf_dir = os.path.join(data_dir, 'pdfs')\n",
    "# *********************** [END] data directory config *********************** #\n",
    "\n",
    "\n",
    "# ********************** [START] LLM data config **************************** #\n",
    "from helpers.files import file_exists\n",
    "\n",
    "collection_name = 'pdf-source'\n",
    "load_documents = True\n",
    "if file_exists(chroma_db_pdf_source_dir):\n",
    "    load_documents = False\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 150\n",
    "# *********************** [END] LLM data config ***************************** #\n",
    "\n",
    "\n",
    "# ******************* [START] vector db retriever config ******************** #\n",
    "# experiment with:\n",
    "# - mmr\n",
    "# - similarity\n",
    "db_search_type = \"similarity\"\n",
    "db_search_kwargs = {\"k\": 5}\n",
    "# ********************* [END] vector db retriever config ******************** #\n",
    "\n",
    "\n",
    "# *********************** [START] PDF tool config *************************** #\n",
    "# the PDF file to create embeddings for and load into ChromaDB\n",
    "pdf_file_path = os.path.join(pdf_dir, 'Pool_Billiards_Rules_of_Play_01MAR2016.pdf')\n",
    "\n",
    "# tool config for the langchain react agent\n",
    "tool_name = \"Read the World Pool-Billiard Association Rules of Play\"\n",
    "tool_description = \"use this to read the rules of pool billiards\"\n",
    "# *********************** [END] PDF tool config ***************************** #\n",
    "\n",
    "\n",
    "# *********************** [START] LLM parameter config ********************** #\n",
    "# Vertex AI model to use for the LLM\n",
    "model_name='text-bison@002'\n",
    "\n",
    "# maximum number of model responses generated per prompt\n",
    "candidate_count = 1\n",
    "\n",
    "# determines the maximum amount of text output from one prompt.\n",
    "# a token is approximately four characters.\n",
    "max_output_tokens = 2048\n",
    "\n",
    "# temperature controls the degree of randomness in token selection.\n",
    "# lower temperatures are good for prompts that expect a true or\n",
    "# correct response, while higher temperatures can lead to more\n",
    "# diverse or unexpected results. With a temperature of 0 the highest\n",
    "# probability token is always selected. for most use cases, try\n",
    "# starting with a temperature of 0.2.\n",
    "temperature = 0.1\n",
    "\n",
    "# top-p changes how the model selects tokens for output. Tokens are\n",
    "# selected from most probable to least until the sum of their\n",
    "# probabilities equals the top-p value. For example, if tokens A, B, and C\n",
    "# have a probability of .3, .2, and .1 and the top-p value is .5, then the\n",
    "# model will select either A or B as the next token (using temperature).\n",
    "# the default top-p value is .8.\n",
    "top_p = 0.8\n",
    "\n",
    "# top-k changes how the model selects tokens for output.\n",
    "# a top-k of 1 means the selected token is the most probable among\n",
    "# all tokens in the modelâ€™s vocabulary (also called greedy decoding),\n",
    "# while a top-k of 3 means that the next token is selected from among\n",
    "# the 3 most probable tokens (using temperature).\n",
    "top_k = 40\n",
    "\n",
    "# how verbose the llm and langchain agent is when thinking\n",
    "# through a prompt. you're going to want this set to True\n",
    "# for development so you can debug its thought process\n",
    "verbose = True\n",
    "# *********************** [END] LLM parameter config ************************ #\n",
    "\n",
    "\n",
    "# ********************** [START] Configuration Checks *********************** #\n",
    "if not project:\n",
    "    raise Exception('GCP_PROJECT environment variable not set')\n",
    "# *********************** [END] Configuration Checks ************************ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d0ba7",
   "metadata": {},
   "source": [
    "## Import and Initialize Vertex AI Client\n",
    "\n",
    "This will complain about not having cuda drivers and the GPU not being used. You can safely ignore that. If you want to use the GPU, that's possible in Linux with Docker, but you'll need to set up a non-containerized development environment to use GPUs with MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076f2cc2-ed7b-4381-ad68-e6c21b811f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 00:11:55.697689: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 00:11:55.699449: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 00:11:55.718301: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 00:11:55.718337: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 00:11:55.718354: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 00:11:55.722750: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 00:11:55.723090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 00:11:56.178025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.38.1\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=project, location=location)\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fc80c",
   "metadata": {},
   "source": [
    "## Import LangChain\n",
    "\n",
    "This doesn't actually initialize anything, it just lets us print the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87c2a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.0.350\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b9a7c",
   "metadata": {},
   "source": [
    "## Configure LLM with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac3671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=model_name,\n",
    "    max_output_tokens=max_output_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    verbose=verbose,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9be59",
   "metadata": {},
   "source": [
    "## Initialize Embeddings Function with Vertex AI\n",
    "\n",
    "There are other options for creating embeddings. I was interested in sticking with Google products here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf3ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.vertexai.VertexAIEmbeddings.html\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    requests_per_minute=embeddings_requests_per_minute,\n",
    "    num_instances_per_batch=embeddings_num_instances_per_batch,\n",
    "    model_name = \"textembedding-gecko@latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b981bd",
   "metadata": {},
   "source": [
    "## Load PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e651df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d4e30",
   "metadata": {},
   "source": [
    "## Chunk the PDFs into smaller pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d1c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count: 25\n",
      "Transformed document count: 160\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "\n",
    "transformed_docs = split_docs(\n",
    "  documents,\n",
    "  chunk_size=chunk_size,\n",
    "  chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "print(f'Document count: {len(documents)}')\n",
    "print(f'Transformed document count: {len(transformed_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe474f",
   "metadata": {},
   "source": [
    "## Create Embeddings Database\n",
    "\n",
    "This is written with persistence and will not re-create the database if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24824dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "if load_documents:\n",
    "  # https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma.from_documents\n",
    "  db = Chroma.from_documents(\n",
    "    transformed_docs,\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=chroma_db_pdf_source_dir,\n",
    "  )\n",
    "else:\n",
    "  db = Chroma(\n",
    "    persist_directory=chroma_db_pdf_source_dir,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e0e19",
   "metadata": {},
   "source": [
    "## Create Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c23a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "  search_type=db_search_type,\n",
    "  search_kwargs=db_search_kwargs,\n",
    ")\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6031ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who shoots first in pool?', 'result': ' The players lag to determine who shoots first.'}\n",
      "{'query': 'What does the following mean in simple terms?  The players lag to determine who shoots first.', 'result': ' The players shoot at the same time to see who can get their ball closer to the head cushion. Whoever gets their ball closer gets to shoot first.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Who shoots first in pool?\"\n",
    "result = retrieval_qa({'query': query})\n",
    "print(result)\n",
    "\n",
    "query = f\"What does the following mean in simple terms? {result['result']}\"\n",
    "result = retrieval_qa({'query': query})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd78f53",
   "metadata": {},
   "source": [
    "## Build chain\n",
    "\n",
    "This will create a chat bot interface to talk about the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b2e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "# https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b53f75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:5000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:5000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_response(message, history):\n",
    "  return chain.invoke(message)\n",
    "\n",
    "demo = gr.ChatInterface(chat_response)\n",
    "\n",
    "demo.launch(\n",
    "  server_name=\"0.0.0.0\",\n",
    "  server_port=5000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
